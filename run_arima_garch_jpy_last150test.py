#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ARIMA+GARCHæ—¶é—´åºåˆ—é¢„æµ‹ç³»ç»Ÿ - å¤šæ­¥é¢„æµ‹ç‰ˆæœ¬ (è‹±é•‘å…‘äººæ°‘å¸)
================================================================

åŠŸèƒ½è¯´æ˜:
--------
è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè‹±é•‘å…‘äººæ°‘å¸æ±‡ç‡é¢„æµ‹çš„ARIMA+GARCHæ¨¡å‹ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§é¢„æµ‹æ­¥é•¿ã€‚

ä¸»è¦ç‰¹æ€§:
--------
1. å¤šæ­¥é¢„æµ‹æ”¯æŒ: 1å¤©ã€24å¤©ã€30å¤©ã€60å¤©ã€90å¤©ã€180å¤©é¢„æµ‹
2. å¤šç§æ¨¡å‹ç±»å‹:
   - Naive Baseline: æœ´ç´ åŸºçº¿æ¨¡å‹
   - Pure ARMA: çº¯æ—¶é—´åºåˆ—æ¨¡å‹  
   - GARCHæ—æ¨¡å‹: ARCHã€GARCHã€EGARCHã€GARCH-Mã€GJR-GARCHã€TGARCH
3. ä¸¤ç§å‡å€¼æ¨¡å‹:
   - Constantå‡å€¼: ä½¿ç”¨å¸¸æ•°å‡å€¼
   - ARå‡å€¼: ä½¿ç”¨è‡ªå›å½’å‡å€¼
4. æ»šåŠ¨çª—å£é¢„æµ‹: ä½¿ç”¨åŠ¨æ€15%æµ‹è¯•é›†
5. å…¨é¢è¯Šæ–­: Ljung-Boxæ£€éªŒã€æ­£æ€æ€§æ£€éªŒã€æ®‹å·®åˆ†æ

è¾“å‡ºç»“æœ:
--------
1. æ¨¡å‹å‚æ•°æ–‡ä»¶ (model_params_*.json)
2. å®Œæ•´ç»“æœæ•°æ® (results_*.pkl)
3. è¯„ä¼°æŒ‡æ ‡æ±‡æ€»è¡¨ (summary_table_*.csv)

ä½¿ç”¨æ–¹æ³•:
--------
python run_arima_garch_jpy_last150test.py --root_path ./dataset/ --data_path è‹±é•‘å…‘äººæ°‘å¸_short_series.csv --target rate

ä½œè€…: æ—¶é—´åºåˆ—é¢„æµ‹ç³»ç»Ÿ
ç‰ˆæœ¬: è‹±é•‘äººæ°‘å¸ä¼˜åŒ–ç‰ˆ v3.0
æ—¥æœŸ: 2024
"""

import os
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
from arch import arch_model
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
import time
from tqdm import tqdm
import pickle
import argparse
import sys
from matplotlib.font_manager import FontProperties
import seaborn as sns
from scipy import stats
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.stats.diagnostic import acorr_ljungbox
import json

# å®Œå…¨ç¦ç”¨æ‰€æœ‰è­¦å‘Š
warnings.filterwarnings('ignore')
import logging
logging.getLogger().setLevel(logging.ERROR)

# ç¦ç”¨matplotlibè­¦å‘Š
plt.rcParams.update({'figure.max_open_warning': 0})

def load_and_prepare_data(root_path, data_path, target_col_name, log_return_col_name='log_return'):
    """
    æ•°æ®åŠ è½½å’Œé¢„å¤„ç†å‡½æ•°
    
    åŠŸèƒ½è¯´æ˜:
    --------
    1. åŠ è½½æ±‡ç‡æ•°æ®æ–‡ä»¶
    2. å¤„ç†æ—¥æœŸåˆ—å¹¶æ’åº
    3. è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡ (log return)
    4. æ•°æ®æ¸…æ´—å’ŒéªŒè¯
    
    å‚æ•°:
    ----
    root_path : str
        æ•°æ®æ–‡ä»¶æ ¹ç›®å½•è·¯å¾„
    data_path : str  
        æ•°æ®æ–‡ä»¶å
    target_col_name : str
        ç›®æ ‡å˜é‡åˆ—å (å¦‚'rate'è¡¨ç¤ºæ±‡ç‡)
    log_return_col_name : str, é»˜è®¤='log_return'
        å¯¹æ•°æ”¶ç›Šç‡åˆ—å
        
    è¿”å›:
    ----
    pd.DataFrame
        åŒ…å«æ—¥æœŸã€åŸå§‹æ±‡ç‡å’Œå¯¹æ•°æ”¶ç›Šç‡çš„æ¸…æ´—åDataFrame
        
    å¼‚å¸¸:
    ----
    ValueError
        å½“ç›®æ ‡åˆ—ä¸å­˜åœ¨ã€æ•°æ®ç±»å‹é”™è¯¯æˆ–å¤„ç†åæ•°æ®ä¸ºç©ºæ—¶æŠ›å‡º
        
    æ•°å­¦åŸç†:
    --------
    å¯¹æ•°æ”¶ç›Šç‡è®¡ç®—: r_t = ln(P_t / P_{t-1}) = ln(P_t) - ln(P_{t-1})
    å…¶ä¸­ P_t ä¸º t æ—¶åˆ»çš„ä»·æ ¼ï¼Œr_t ä¸º t æ—¶åˆ»çš„å¯¹æ•°æ”¶ç›Šç‡
    """
    df = pd.read_csv(os.path.join(root_path, data_path))
    
    date_col = 'date' if 'date' in df.columns else 'Date'
    df.rename(columns={date_col: 'date'}, inplace=True)
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values(by='date').reset_index(drop=True)

    if target_col_name not in df.columns:
        raise ValueError(f"ç›®æ ‡åˆ— '{target_col_name}' åœ¨æ•°æ®ä¸­æœªæ‰¾åˆ°.")
    
    if not pd.api.types.is_numeric_dtype(df[target_col_name]):
        raise ValueError(f"ç›®æ ‡åˆ— '{target_col_name}' å¿…é¡»æ˜¯æ•°å€¼ç±»å‹.")
    if (df[target_col_name] <= 0).any():
        df = df[df[target_col_name] > 0].reset_index(drop=True)
        if df.empty:
            raise ValueError("ç§»é™¤é›¶æˆ–è´Ÿæ•°å€¼åï¼Œæ•°æ®ä¸ºç©ºã€‚")

    # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
    df[log_return_col_name] = np.log(df[target_col_name] / df[target_col_name].shift(1))
    
    # ç§»é™¤ç¬¬ä¸€ä¸ªNaNå€¼ï¼ˆç”±äºshiftæ“ä½œäº§ç”Ÿï¼‰
    df.dropna(subset=[log_return_col_name], inplace=True)
    df.reset_index(drop=True, inplace=True) #ç¡®ä¿ç´¢å¼•é‡ç½®
    
    if df.empty:
        raise ValueError("è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡å¹¶ç§»é™¤NaNåï¼Œæ•°æ®ä¸ºç©ºã€‚")
        
    return df

def perform_stationarity_test(series, series_name="åºåˆ—"):
    """å¯¹ç»™å®šåºåˆ—æ‰§è¡ŒADFå¹³ç¨³æ€§æ£€éªŒå¹¶æ‰“å°ç»“æœã€‚"""
    print(f"\nå¯¹ {series_name} è¿›è¡ŒADFå¹³ç¨³æ€§æ£€éªŒ:")
    # ç¡®ä¿åºåˆ—ä¸­æ²¡æœ‰NaNå€¼ä¼ é€’ç»™adfuller
    adf_result = adfuller(series.dropna()) 
    print(f'ADF ç»Ÿè®¡é‡: {adf_result[0]}')
    print(f'p-å€¼: {adf_result[1]}')
    print('ä¸´ç•Œå€¼:')
    for key, value in adf_result[4].items():
        print(f'    {key}: {value}')
    if adf_result[1] <= 0.05:
        print(f"ç»“è®º: p-å€¼ ({adf_result[1]:.4f}) <= 0.05, æ‹’ç»åŸå‡è®¾ï¼Œ{series_name} å¤§æ¦‚ç‡æ˜¯å¹³ç¨³çš„ã€‚")
    else:
        print(f"ç»“è®º: p-å€¼ ({adf_result[1]:.4f}) > 0.05, ä¸èƒ½æ‹’ç»åŸå‡è®¾ï¼Œ{series_name} å¤§æ¦‚ç‡æ˜¯éå¹³ç¨³çš„ã€‚")
    print("-" * 50)

def perform_model_diagnostics(residuals, model_name="Unknown"):
    """
    æ‰§è¡Œå…¨é¢çš„æ¨¡å‹è¯Šæ–­æ£€éªŒ
    Args:
        residuals: æ®‹å·®åºåˆ—
        model_name: æ¨¡å‹åç§°
    Returns:
        åŒ…å«å„ç§è¯Šæ–­ç»“æœçš„å­—å…¸
    """
    try:
        # 1. åŸºæœ¬ç»Ÿè®¡é‡
        stats_info = {
            'mean': float(np.mean(residuals)),
            'std': float(np.std(residuals)),
            'skewness': float(stats.skew(residuals)),
            'kurtosis': float(stats.kurtosis(residuals, fisher=True) + 3),  # è½¬æ¢ä¸ºæ™®é€šå³°åº¦
            'min': float(np.min(residuals)),
            'max': float(np.max(residuals)),
            'n_samples': len(residuals)
        }
        
        # 2. æ­£æ€æ€§æ£€éªŒ
        normality_tests = {
            'jarque_bera': {
                'statistic': float(stats.jarque_bera(residuals)[0]),
                'p_value': float(stats.jarque_bera(residuals)[1])
            },
            'shapiro': {
                'statistic': float(stats.shapiro(residuals)[0]),
                'p_value': float(stats.shapiro(residuals)[1])
            }
        }
        
        # 3. åºåˆ—ç›¸å…³æ€§æ£€éªŒï¼ˆåŸå§‹æ®‹å·®ï¼‰
        lb_tests = {}
        for lag in [5, 10, 15, 20]:
            lb_test = acorr_ljungbox(residuals, lags=[lag], return_df=True)
            lb_tests[f'lag_{lag}'] = {
                'statistic': float(lb_test['lb_stat'].values[0]),
                'p_value': float(lb_test['lb_pvalue'].values[0]) 
            }
        
        # 4. ARCHæ•ˆåº”æ£€éªŒï¼ˆå¯¹æ®‹å·®å¹³æ–¹è¿›è¡ŒLBæ£€éªŒï¼‰
        squared_resid = residuals ** 2
        arch_tests = {}
        for lag in [5, 10, 15]:
            arch_test = acorr_ljungbox(squared_resid, lags=[lag], return_df=True)
            arch_tests[f'lag_{lag}'] = {
                'statistic': float(arch_test['lb_stat'].values[0]),
                'p_value': float(arch_test['lb_pvalue'].values[0])
            }
        
        # 5. è‡ªç›¸å…³ç³»æ•°
        acf_values = pd.Series(residuals).autocorr(lag=1)  # ä¸€é˜¶è‡ªç›¸å…³ç³»æ•°
        
        # 6. å¹³ç¨³æ€§æ£€éªŒ
        try:
            adf_test = adfuller(residuals)
            stationarity_test = {
                'adf_statistic': float(adf_test[0]),
                'p_value': float(adf_test[1]),
                'critical_values': {str(key): float(val) for key, val in adf_test[4].items()}
            }
        except:
            stationarity_test = None
        
        return {
            'model_name': model_name,
            'basic_stats': stats_info,
            'normality_tests': normality_tests,
            'ljung_box_tests': lb_tests,
            'arch_effect_tests': arch_tests,
            'first_order_autocorr': float(acf_values),
            'stationarity_test': stationarity_test
        }
        
    except Exception as e:
        return None

def train_and_forecast_arima_garch(scaled_log_return_series_history, original_log_return_series, model_type, seq_len, pred_len, p_ar=1, q_ma=0, save_params=False):
    """
    ARIMA+GARCHæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹æ ¸å¿ƒå‡½æ•°
    
    åŠŸèƒ½è¯´æ˜:
    --------
    1. æ”¯æŒå¤šç§GARCHæ—æ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
    2. å®ç°å¤šæ­¥é¢„æµ‹åŠŸèƒ½
    3. è‡ªåŠ¨æ¨¡å‹å‚æ•°ä¼°è®¡å’Œè¯Šæ–­
    4. å¼‚å¸¸å¤„ç†å’ŒARMAæ¨¡å‹å›é€€æœºåˆ¶
    
    æ”¯æŒçš„æ¨¡å‹ç±»å‹:
    -------------
    1. ARCHç±»: 
       - ARCH(1)_AR: ARå‡å€¼ + ARCH(1)æ–¹å·®
       - ARCH(1)_Const: å¸¸æ•°å‡å€¼ + ARCH(1)æ–¹å·®
    2. GARCHç±»:
       - GARCH(1,1)_AR: ARå‡å€¼ + GARCH(1,1)æ–¹å·®
       - GARCH(1,1)_Const: å¸¸æ•°å‡å€¼ + GARCH(1,1)æ–¹å·®
    3. EGARCHç±»:
       - EGARCH(1,1)_AR: ARå‡å€¼ + EGARCH(1,1)æ–¹å·® (éå¯¹ç§°æ•ˆåº”)
       - EGARCH(1,1)_Const: å¸¸æ•°å‡å€¼ + EGARCH(1,1)æ–¹å·®
    4. GARCH-Mç±»:
       - GARCH-M(1,1)_AR: ARå‡å€¼-æ–¹å·® + GARCH(1,1)
       - GARCH-M(1,1)_Const: å¸¸æ•°å‡å€¼-æ–¹å·® + GARCH(1,1)
    5. GJR-GARCHç±»:
       - GJR-GARCH(1,1)_AR: ARå‡å€¼ + GJR-GARCH(1,1) (é˜ˆå€¼GARCH)
    6. TGARCHç±»:
       - TGARCH(1,1)_AR: ARå‡å€¼ + TGARCH(1,1) (é˜ˆå€¼GARCH)
    
    å‚æ•°:
    ----
    scaled_log_return_series_history : array-like
        æ ‡å‡†åŒ–åçš„å¯¹æ•°æ”¶ç›Šç‡å†å²åºåˆ— (ç”¨äºARMAå›é€€)
    original_log_return_series : array-like
        åŸå§‹å¯¹æ•°æ”¶ç›Šç‡åºåˆ— (ç”¨äºGARCHæ—æ¨¡å‹ï¼Œé¿å…æ ‡å‡†åŒ–å½±å“)
    model_type : str
        æ¨¡å‹ç±»å‹æ ‡è¯†ç¬¦
    seq_len : int
        è®­ç»ƒåºåˆ—é•¿åº¦ (æ»šåŠ¨çª—å£å¤§å°)
    pred_len : int
        é¢„æµ‹æ­¥é•¿ (1, 24, 30, 60, 90, 180å¤©)
    p_ar : int, é»˜è®¤=1
        AR(è‡ªå›å½’)é¡¹é˜¶æ•°
    q_ma : int, é»˜è®¤=0
        MA(ç§»åŠ¨å¹³å‡)é¡¹é˜¶æ•°
    save_params : bool, é»˜è®¤=False
        æ˜¯å¦ä¿å­˜æ¨¡å‹å‚æ•°å’Œè¯Šæ–­ç»“æœ
        
    è¿”å›:
    ----
    tuple : (predictions, model_params, diagnostics_result)
        predictions : ndarray
            pred_lenæ­¥çš„é¢„æµ‹å€¼
        model_params : dict or None
            æ¨¡å‹å‚æ•°å­—å…¸ (å½“save_params=Trueæ—¶)
        diagnostics_result : dict or None
            æ¨¡å‹è¯Šæ–­ç»“æœ (å½“save_params=Trueæ—¶)
            
    æ•°å­¦æ¨¡å‹:
    --------
    GARCH(1,1)æ¨¡å‹:
        å‡å€¼æ–¹ç¨‹: r_t = Î¼ + Ï†â‚r_{t-1} + Îµáµ—
        æ–¹å·®æ–¹ç¨‹: ÏƒÂ²áµ— = Ï‰ + Î±â‚ÎµÂ²_{t-1} + Î²â‚ÏƒÂ²_{t-1}
        
    EGARCH(1,1)æ¨¡å‹:
        ln(ÏƒÂ²áµ—) = Ï‰ + Î±â‚|Îµ_{t-1}|/Ïƒ_{t-1} + Î³â‚Îµ_{t-1}/Ïƒ_{t-1} + Î²â‚ln(ÏƒÂ²_{t-1})
        
    GARCH-Mæ¨¡å‹:
        r_t = Î¼ + Î»Ïƒáµ— + Ï†â‚r_{t-1} + Îµáµ— (å‡å€¼ä¸­åŒ…å«æ¡ä»¶æ–¹å·®é¡¹)
    """
    train_seq = original_log_return_series[-seq_len:]  # ä½¿ç”¨åŸå§‹åºåˆ—
    garch_model_instance = None
    model_params = None
    diagnostics_result = None

    try:
        if 'GARCH-M' in model_type:
            if model_type == 'GARCH-M(1,1)_AR':
                base_model = arch_model(train_seq, mean='Zero', vol='GARCH', p=1, q=1)
                base_res = base_model.fit(disp='off', show_warning=False)
                conditional_vol = base_res.conditional_volatility
                idx = train_seq.index if hasattr(train_seq, 'index') else pd.RangeIndex(start=len(train_seq) - len(conditional_vol), stop=len(train_seq))
                x_for_arx = pd.DataFrame({'vol': conditional_vol}, index=idx[-len(conditional_vol):])
                
                arx_model = arch_model(train_seq, x=x_for_arx, mean='ARX', lags=p_ar, vol='Constant')
                arx_res = arx_model.fit(disp='off', show_warning=False)
                
                if save_params:
                    model_params = {
                        'mean_equation': arx_res.params.to_dict(),
                        'variance_equation': base_res.params.to_dict()
                    }
                    # è¿›è¡Œæ¨¡å‹è¯Šæ–­
                    diagnostics_result = perform_model_diagnostics(arx_res.resid, model_type)
                
                vol_forecasts_obj = base_res.forecast(horizon=pred_len)
                predicted_variance = vol_forecasts_obj.variance.values[0, :pred_len]
                predicted_std_dev = np.sqrt(np.maximum(predicted_variance, 0))
                x_forecast_df = pd.DataFrame({'vol': predicted_std_dev}, index=np.arange(pred_len))
                mean_forecasts_obj = arx_res.forecast(horizon=pred_len, x=x_forecast_df)
                return mean_forecasts_obj.mean.values[0, :pred_len], model_params, diagnostics_result
            else:  # GARCH-M(1,1)_Const
                try:
                    base_garch = arch_model(train_seq, mean='Constant', vol='GARCH', p=1, q=1)
                    base_res = base_garch.fit(disp='off', show_warning=False)
                    
                    if save_params:
                        model_params = {
                            'mean_equation': {'mu': float(base_res.params['mu'])},
                            'variance_equation': {k: float(v) for k, v in base_res.params.items() if k != 'mu'}
                        }
                        # è¿›è¡Œæ¨¡å‹è¯Šæ–­
                        diagnostics_result = perform_model_diagnostics(base_res.resid, model_type)
                    
                    conditional_vol = base_res.conditional_volatility
                    const_param = base_res.params['mu']
                    
                    vol_forecasts = base_res.forecast(horizon=pred_len)
                    predicted_variance = vol_forecasts.variance.values[0, :pred_len]
                    predicted_std_dev = np.sqrt(np.maximum(predicted_variance, 0))
                    
                    risk_premium_coef = 0.1
                    garch_m_forecasts = const_param + risk_premium_coef * predicted_std_dev
                    return garch_m_forecasts, model_params, diagnostics_result
                except Exception as e:
                    print(f"è­¦å‘Š: è‡ªå®šä¹‰GARCH-Må®ç°å¤±è´¥ ({str(e)}),ä½¿ç”¨æ ‡å‡†GARCH(1,1)ã€‚")
                    standard_garch = arch_model(train_seq, mean='Constant', vol='GARCH', p=1, q=1)
                    res = standard_garch.fit(disp='off', show_warning=False)
                    forecast_obj = res.forecast(horizon=pred_len)
                    return forecast_obj.mean.values[0, :pred_len], None, None
                
        else: 
            # æ ¹æ®æ¨¡å‹ç±»å‹å’Œå‡å€¼è®¾ç½®é€‰æ‹©åˆé€‚çš„é…ç½®
            if model_type == 'ARCH(1)_AR':
                garch_model_instance = arch_model(train_seq, mean='AR', lags=p_ar, vol='ARCH', p=1, q=0)
            elif model_type == 'ARCH(1)_Const':
                garch_model_instance = arch_model(train_seq, mean='Constant', vol='ARCH', p=1, q=0)
            elif model_type == 'GARCH(1,1)_AR':
                garch_model_instance = arch_model(train_seq, mean='AR', lags=p_ar, vol='GARCH', p=1, o=0, q=1)
            elif model_type == 'GARCH(1,1)_Const':
                garch_model_instance = arch_model(train_seq, mean='Constant', vol='GARCH', p=1, o=0, q=1)
            elif model_type == 'EGARCH(1,1)_AR':
                garch_model_instance = arch_model(train_seq, mean='AR', lags=p_ar, vol='EGARCH', p=1, o=1, q=1)
            elif model_type == 'EGARCH(1,1)_Const':
                garch_model_instance = arch_model(train_seq, mean='Constant', vol='EGARCH', p=1, o=1, q=1)
            elif model_type == 'GJR-GARCH(1,1)_AR':
                garch_model_instance = arch_model(train_seq, mean='AR', lags=p_ar, vol='GARCH', p=1, o=1, q=1)
            elif model_type == 'GJR-GARCH(1,1)_Const':
                garch_model_instance = arch_model(train_seq, mean='Constant', vol='GARCH', p=1, o=1, q=1)
            elif model_type == 'TGARCH(1,1)_AR':
                garch_model_instance = arch_model(train_seq, mean='AR', lags=p_ar, vol='GARCH', p=1, o=1, q=1, power=1.0)
            elif model_type == 'TGARCH(1,1)_Const':
                garch_model_instance = arch_model(train_seq, mean='Constant', vol='GARCH', p=1, o=1, q=1, power=1.0)
            else:
                raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
            
            try:
                fit_update_freq = 5 if 'EGARCH' in model_type or 'GJR-GARCH' in model_type or 'TGARCH' in model_type else 1
                results = garch_model_instance.fit(disp='off', show_warning=False, update_freq=fit_update_freq)
                
                if save_params:
                    # åˆ†ç¦»å‡å€¼æ–¹ç¨‹å’Œæ–¹å·®æ–¹ç¨‹çš„å‚æ•°
                    mean_params = {}
                    variance_params = {}
                    
                    for param_name, param_value in results.params.items():
                        param_value_float = float(param_value)
                        if any(x in param_name.lower() for x in ['omega', 'alpha', 'beta', 'gamma']):
                            variance_params[param_name] = param_value_float
                        else:
                            mean_params[param_name] = param_value_float
                    
                    model_params = {
                        'mean_equation': mean_params,
                        'variance_equation': variance_params,
                        'ar_order': p_ar,
                        'ma_order': q_ma
                    }
                    
                    # è¿›è¡Œå®Œæ•´çš„æ¨¡å‹è¯Šæ–­
                    diagnostics_result = perform_model_diagnostics(results.resid, model_type)
                
                forecast_obj = results.forecast(horizon=pred_len)
                return forecast_obj.mean.values[0, :pred_len], model_params, diagnostics_result
                
            except Exception as e_garch_fit:
                print(f"è­¦å‘Š: {model_type} æ¨¡å‹æ‹Ÿåˆæˆ–é¢„æµ‹å¤±è´¥ ({str(e_garch_fit)})ã€‚å°è¯•ä½¿ç”¨çº¯ARMAæ¨¡å‹ã€‚")
                try:
                    arima_model = ARIMA(scaled_log_return_series_history[-seq_len:], order=(p_ar, 0, q_ma))
                    arima_results = arima_model.fit()
                    forecast_val = arima_results.forecast(steps=pred_len)
                    return np.array(forecast_val.values if isinstance(forecast_val, pd.Series) else forecast_val).flatten()[:pred_len], None, None
                except Exception as e_arima_fallback:
                    print(f"è­¦å‘Š: {model_type}çš„çº¯ARMA({p_ar},0,{q_ma})å›é€€å¤±è´¥: {e_arima_fallback}ã€‚è¿”å›é›¶é¢„æµ‹ã€‚")
                    return np.zeros(pred_len), None, None
                    
    except Exception as e_garch_m:
        print(f"è­¦å‘Š: {model_type} æ¨¡å‹æ‹Ÿåˆæˆ–é¢„æµ‹å¤±è´¥ ({str(e_garch_m)})ã€‚å°è¯•ä½¿ç”¨çº¯ARMAæ¨¡å‹ã€‚")
        try:
            arima_model = ARIMA(scaled_log_return_series_history[-seq_len:], order=(p_ar, 0, q_ma))
            arima_results = arima_model.fit()
            forecast_val = arima_results.forecast(steps=pred_len)
            return np.array(forecast_val.values if isinstance(forecast_val, pd.Series) else forecast_val).flatten()[:pred_len], None, None
        except Exception as e_arima:
            print(f"è­¦å‘Š: {model_type}çš„çº¯ARMA({p_ar},0,{q_ma})å›é€€å¤±è´¥: {e_arima}ã€‚è¿”å›é›¶é¢„æµ‹ã€‚")
            return np.zeros(pred_len), None, None

    return forecast_obj.mean.values[0, :pred_len], model_params, diagnostics_result

def evaluate_model(y_true_prices, y_pred_prices):
    """
    æ¨¡å‹è¯„ä¼°å‡½æ•° - ä¿®å¤ç‰ˆæœ¬
    
    ä¿®å¤è¯´æ˜:
    --------
    1. æ·»åŠ æ•°ç»„é•¿åº¦ä¸€è‡´æ€§æ£€æŸ¥
    2. å¤„ç†NaN/Infå€¼
    3. ç¡®ä¿æ•°ç»„å½¢çŠ¶æ­£ç¡®
    """
    y_true_prices = np.asarray(y_true_prices).flatten()
    y_pred_prices = np.asarray(y_pred_prices).flatten()
    
    # ä¿®å¤: æ·»åŠ é•¿åº¦ä¸€è‡´æ€§æ£€æŸ¥
    if len(y_true_prices) != len(y_pred_prices):
        min_length = min(len(y_true_prices), len(y_pred_prices))
        y_true_prices = y_true_prices[:min_length]
        y_pred_prices = y_pred_prices[:min_length]
    
    # ä¿®å¤: æ£€æŸ¥æ•°ç»„æ˜¯å¦ä¸ºç©º
    if len(y_true_prices) == 0 or len(y_pred_prices) == 0:
        return {'MSE': float('inf'), 'RMSE': float('inf'), 'MAE': float('inf'), 'MAX_AE': float('inf'), 'MAPE': float('inf'), 'R2': -float('inf')}
    
    if not np.all(np.isfinite(y_pred_prices)):
        finite_preds = y_pred_prices[np.isfinite(y_pred_prices)]
        mean_finite_pred = np.mean(finite_preds) if len(finite_preds) > 0 else (np.mean(y_true_prices) if len(y_true_prices) > 0 else 0)
        y_pred_prices = np.nan_to_num(y_pred_prices, nan=mean_finite_pred, posinf=1e12, neginf=-1e12)

    mse = mean_squared_error(y_true_prices, y_pred_prices)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true_prices, y_pred_prices)
    max_ae = np.max(np.abs(y_true_prices - y_pred_prices))  # æ·»åŠ MAX AEæŒ‡æ ‡
    r2 = r2_score(y_true_prices, y_pred_prices)
    
    mask = np.abs(y_true_prices) > 1e-9
    mape = np.mean(np.abs((y_true_prices[mask] - y_pred_prices[mask]) / y_true_prices[mask])) * 100 if np.sum(mask) > 0 else np.nan
    
    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAX_AE': max_ae, 'MAPE': mape, 'R2': r2}

def run_naive_baseline_forecast(data_df, original_price_col_name, pred_len, step_size, test_set_ratio=0.15):
    """
    æœ´ç´ åŸºçº¿é¢„æµ‹å‡½æ•° - ä¿®å¤ç‰ˆæœ¬
    
    ä¿®å¤è¯´æ˜:
    --------
    1. ç¡®ä¿é¢„æµ‹çª—å£ä¸è¶…å‡ºæ•°æ®è¾¹ç•Œ
    2. çœŸå®å€¼å’Œé¢„æµ‹å€¼æ•°ç»„é•¿åº¦ä¸¥æ ¼åŒ¹é…
    3. æ­£ç¡®å¤„ç†å¤šæ­¥é¢„æµ‹çš„è¾¹ç•Œæƒ…å†µ
    """
    original_prices = data_df[original_price_col_name].values
    num_total_points = len(original_prices)
    fixed_test_set_size = int(num_total_points * test_set_ratio)

    if num_total_points <= fixed_test_set_size:
        return np.array([]), np.array([])

    test_start_idx = num_total_points - fixed_test_set_size
    all_true_price_levels_collected = []
    all_predicted_price_levels_collected = []

    # ä¿®å¤: ç¡®ä¿å¾ªç¯èŒƒå›´ä¸ä¼šå¯¼è‡´è¶Šç•Œ
    # æœ€åä¸€ä¸ªæœ‰æ•ˆçš„é¢„æµ‹èµ·å§‹ä½ç½®åº”è¯¥æ˜¯ num_total_points - pred_len
    max_start_idx = min(num_total_points - pred_len, num_total_points - 1)
    
    for i in tqdm(range(test_start_idx, max_start_idx + 1, step_size), 
                 desc=f"Naive Baseline é¢„æµ‹è¿›åº¦ ({pred_len}å¤©)", file=sys.stderr):
        if i < 1 : continue 
        
        # ä¿®å¤: ç¡®ä¿ä¸ä¼šè¶…å‡ºæ•°æ®è¾¹ç•Œ
        end_idx = min(i + pred_len, num_total_points)
        actual_price_levels_this_window = original_prices[i : end_idx] 
        
        # ä¿®å¤: åªæœ‰å½“å®é™…æ•°æ®é•¿åº¦ç­‰äºpred_lenæ—¶æ‰å¤„ç†
        if len(actual_price_levels_this_window) != pred_len:
            continue
            
        value_from_previous_day = original_prices[i-1] 
        predicted_price_levels_this_window = [value_from_previous_day] * pred_len
        
        all_true_price_levels_collected.append(actual_price_levels_this_window)
        all_predicted_price_levels_collected.append(predicted_price_levels_this_window)

    if not all_predicted_price_levels_collected:
        return np.array([]), np.array([])

    # ä¿®å¤: ç¡®ä¿è¿æ¥åçš„æ•°ç»„é•¿åº¦ä¸€è‡´
    true_values = np.concatenate(all_true_price_levels_collected)
    pred_values = np.concatenate(all_predicted_price_levels_collected)
    
    # è°ƒè¯•ä¿¡æ¯: æ‰“å°æ•°ç»„é•¿åº¦ä»¥ä¾¿æ£€æŸ¥
    # print(f"æœ´ç´ åŸºçº¿é¢„æµ‹ - çœŸå®å€¼é•¿åº¦: {len(true_values)}, é¢„æµ‹å€¼é•¿åº¦: {len(pred_values)}")
    
    # ä¿®å¤: å¦‚æœä»ç„¶é•¿åº¦ä¸åŒ¹é…ï¼Œæˆªå–åˆ°è¾ƒçŸ­çš„é•¿åº¦
    min_length = min(len(true_values), len(pred_values))
    return true_values[:min_length], pred_values[:min_length]

def rolling_forecast(data_df, original_price_col_name, log_return_col_name, 
                     model_type, seq_len, pred_len, step_size=1, p=1, q=0, test_set_ratio=0.15):
    """
    æ»šåŠ¨çª—å£å¤šæ­¥é¢„æµ‹å‡½æ•°
    
    åŠŸèƒ½è¯´æ˜:
    --------
    1. å®ç°æ»šåŠ¨çª—å£çš„å¤šæ­¥æ—¶é—´åºåˆ—é¢„æµ‹
    2. æ”¯æŒ1åˆ°180å¤©çš„é¢„æµ‹æ­¥é•¿
    3. ä»å¯¹æ•°æ”¶ç›Šç‡é¢„æµ‹é‡æ„åˆ°ä»·æ ¼æ°´å¹³
    4. å®æ—¶å¼‚å¸¸æ£€æµ‹å’Œå¤„ç†
    5. å…¨é¢çš„æ®‹å·®åˆ†æå’ŒLBæ£€éªŒ
    
    é¢„æµ‹æµç¨‹:
    --------
    1. æ•°æ®æ ‡å‡†åŒ–: å¯¹å¯¹æ•°æ”¶ç›Šç‡è¿›è¡ŒZ-scoreæ ‡å‡†åŒ–
    2. æ»šåŠ¨çª—å£: å›ºå®šçª—å£å¤§å°ï¼Œé€æ­¥æ»‘åŠ¨é¢„æµ‹
    3. æ¨¡å‹è®­ç»ƒ: åœ¨æ¯ä¸ªçª—å£ä¸Šè®­ç»ƒGARCHæ¨¡å‹
    4. å¤šæ­¥é¢„æµ‹: é€’å½’/ç›´æ¥é¢„æµ‹æœªæ¥pred_lenæ­¥
    5. ä»·æ ¼é‡æ„: ä»å¯¹æ•°æ”¶ç›Šç‡é¢„æµ‹é‡æ„å®é™…ä»·æ ¼
    6. æ®‹å·®æ”¶é›†: æ”¶é›†æ¯ä¸ªé¢„æµ‹çª—å£çš„æ®‹å·®ç”¨äºè¯Šæ–­
    
    å‚æ•°:
    ----
    data_df : pd.DataFrame
        åŒ…å«æ±‡ç‡å’Œå¯¹æ•°æ”¶ç›Šç‡çš„æ•°æ®æ¡†
    original_price_col_name : str
        åŸå§‹ä»·æ ¼åˆ—å (å¦‚ 'rate')
    log_return_col_name : str
        å¯¹æ•°æ”¶ç›Šç‡åˆ—å
    model_type : str
        GARCHæ¨¡å‹ç±»å‹
    seq_len : int
        æ»šåŠ¨çª—å£é•¿åº¦ (å†å²æ•°æ®é•¿åº¦)
    pred_len : int
        é¢„æµ‹æ­¥é•¿ (1, 24, 30, 60, 90, 180å¤©)
    step_size : int, é»˜è®¤=1
        æ»šåŠ¨æ­¥é•¿
    p : int, é»˜è®¤=1
        ARé¡¹é˜¶æ•°
    q : int, é»˜è®¤=0
        MAé¡¹é˜¶æ•°
    test_set_ratio : float, é»˜è®¤=0.15
        æµ‹è¯•é›†æ¯”ä¾‹
        
    è¿”å›:
    ----
    tuple : (true_values, predictions, model_info, lb_test_result)
        true_values : ndarray
            çœŸå®ä»·æ ¼å€¼ (å±•å¹³çš„æ‰€æœ‰é¢„æµ‹çª—å£)
        predictions : ndarray
            é¢„æµ‹ä»·æ ¼å€¼ (å±•å¹³çš„æ‰€æœ‰é¢„æµ‹çª—å£)
        model_info : dict
            ç¬¬ä¸€ä¸ªçª—å£çš„æ¨¡å‹ä¿¡æ¯å’Œå‚æ•°
        lb_test_result : dict
            å¯¹æ‰€æœ‰é¢„æµ‹æ®‹å·®çš„Ljung-Boxæ£€éªŒç»“æœ
            
    æ•°å­¦åŸç†:
    --------
    ä»·æ ¼é‡æ„å…¬å¼: P_t = P_{t-1} * exp(r_t)
    å…¶ä¸­ P_t ä¸ºtæ—¶åˆ»ä»·æ ¼ï¼Œr_t ä¸ºtæ—¶åˆ»å¯¹æ•°æ”¶ç›Šç‡é¢„æµ‹å€¼
    
    å¤šæ­¥é¢„æµ‹é€šè¿‡é€’å½’åº”ç”¨: P_{t+k} = P_{t+k-1} * exp(r_{t+k})
    """
    log_returns = data_df[log_return_col_name].values
    original_prices = data_df[original_price_col_name].values 

    log_return_scaler = StandardScaler()
    scaled_log_returns = log_return_scaler.fit_transform(log_returns.reshape(-1, 1)).flatten()
    
    num_total_log_points = len(scaled_log_returns)
    fixed_test_set_size = int(num_total_log_points * test_set_ratio)
    test_start_idx_log = num_total_log_points - fixed_test_set_size
    
    if test_start_idx_log < 0: 
        return np.array([]), np.array([]), None, None

    all_true_price_levels_collected = []
    all_predicted_price_levels_collected = []
    first_window_model_info = None
    all_price_level_residuals = []  # æ”¶é›†æ±‡ç‡å°ºåº¦çš„æ®‹å·®ï¼ˆæ¯ä¸ªçª—å£åªå–ç¬¬ä¸€æ­¥é¢„æµ‹çš„æ®‹å·®ï¼‰
    
    extreme_value_threshold = 10.0

    for i in tqdm(range(test_start_idx_log, num_total_log_points - pred_len + 1, step_size),
                 desc=f"{model_type} é¢„æµ‹è¿›åº¦ ({pred_len}å¤©)", file=sys.stderr):
        
        current_history_for_model_input = scaled_log_returns[:i]
        original_log_returns_for_model = log_returns[:i]  # ä½¿ç”¨åŸå§‹å¯¹æ•°æ”¶ç›Šç‡åºåˆ—
        train_seq_actually_used_by_model = current_history_for_model_input[-seq_len:]

        last_actual_price_for_reconstruction = original_prices[i-1] 

        # åœ¨ç¬¬ä¸€ä¸ªçª—å£ä¿å­˜æ¨¡å‹å‚æ•°
        save_params = (i == test_start_idx_log)
        predicted_std_log_returns_raw, model_params, diagnostics_result = train_and_forecast_arima_garch(
            current_history_for_model_input, original_log_returns_for_model, model_type, seq_len, pred_len, 
            p_ar=p, q_ma=q, save_params=save_params
        )

        # ç¡®ä¿ç¬¬ä¸€ä¸ªçª—å£çš„æ¨¡å‹ä¿¡æ¯è¢«æ­£ç¡®ä¿å­˜
        if save_params:
            first_window_model_info = {
                'model_type': model_type,
                'parameters': model_params,
                'diagnostics': diagnostics_result,
                'ar_order': p,
                'ma_order': q
            }

        use_arma_fallback_in_rolling = False

        if not isinstance(predicted_std_log_returns_raw, np.ndarray) or \
           predicted_std_log_returns_raw.shape != (pred_len,):
            print(f"è­¦å‘Š: {model_type} (åœ¨æ»šåŠ¨çª—å£ç´¢å¼• {i}, è®­ç»ƒåºåˆ—å®é™…ä½¿ç”¨é•¿åº¦ {len(train_seq_actually_used_by_model)}) è¿”å›äº†æ ¼å¼/å½¢çŠ¶é”™è¯¯çš„é¢„æµ‹: {predicted_std_log_returns_raw}.")
            use_arma_fallback_in_rolling = True
        elif np.any(np.isnan(predicted_std_log_returns_raw)) or \
             np.any(np.isinf(predicted_std_log_returns_raw)):
            print(f"è­¦å‘Š: {model_type} (åœ¨æ»šåŠ¨çª—å£ç´¢å¼• {i}, è®­ç»ƒåºåˆ—å®é™…ä½¿ç”¨é•¿åº¦ {len(train_seq_actually_used_by_model)}) è¿”å›äº† NaN/Inf é¢„æµ‹: {predicted_std_log_returns_raw}.")
            use_arma_fallback_in_rolling = True
        elif np.any(np.abs(predicted_std_log_returns_raw) > extreme_value_threshold):
            print(f"è­¦å‘Š: {model_type} (åœ¨æ»šåŠ¨çª—å£ç´¢å¼• {i}, è®­ç»ƒåºåˆ—å®é™…ä½¿ç”¨é•¿åº¦ {len(train_seq_actually_used_by_model)}) è¿”å›äº†æç«¯å€¼é¢„æµ‹ (abs > {extreme_value_threshold}): {predicted_std_log_returns_raw}.")
            if len(train_seq_actually_used_by_model) > 0:
                print(f"       è§¦å‘æç«¯å€¼çš„è®­ç»ƒåºåˆ—æ‘˜è¦: len={len(train_seq_actually_used_by_model)}, mean={np.mean(train_seq_actually_used_by_model):.4f}, std={np.std(train_seq_actually_used_by_model):.4f}, min={np.min(train_seq_actually_used_by_model):.4f}, max={np.max(train_seq_actually_used_by_model):.4f}")
            else:
                print(f"       è§¦å‘æç«¯å€¼çš„è®­ç»ƒåºåˆ—ä¸ºç©ºæˆ–é•¿åº¦ä¸è¶³ (ä¼ é€’ç»™æ¨¡å‹çš„åºåˆ—é•¿åº¦ä¸º {len(current_history_for_model_input)}, æ¨¡å‹å†…éƒ¨æˆªå–æœ€å {seq_len})ã€‚")
            use_arma_fallback_in_rolling = True

        if use_arma_fallback_in_rolling:
            print(f"       æ­¤æ­¥éª¤ ({model_type} @ç´¢å¼• {i}) å›é€€åˆ° Pure ARMA({p},{q})ã€‚");
            predicted_std_log_returns, _, _ = train_and_forecast_pure_arma(
                current_history_for_model_input, seq_len, pred_len, p_ar=p, q_ma=q
            )
            if not (isinstance(predicted_std_log_returns, np.ndarray) and \
                    predicted_std_log_returns.shape == (pred_len,) and \
                    not np.any(np.isnan(predicted_std_log_returns)) and \
                    not np.any(np.isinf(predicted_std_log_returns)) and \
                    not np.any(np.abs(predicted_std_log_returns) > extreme_value_threshold)):
                print(f"è­¦å‘Š: Pure ARMA({p},{q}) å›é€€çš„é¢„æµ‹ä»ç„¶æ— æ•ˆ/æç«¯ ({model_type} @ç´¢å¼• {i}): {predicted_std_log_returns}. ä½¿ç”¨é›¶é¢„æµ‹ä»£æ›¿ã€‚")
                predicted_std_log_returns = np.zeros(pred_len)
        else:
            predicted_std_log_returns = predicted_std_log_returns_raw
        
        predicted_log_returns_for_window = log_return_scaler.inverse_transform(
            np.array(predicted_std_log_returns).reshape(-1, 1)
        ).flatten()

        reconstructed_price_forecasts_this_window = []
        current_reconstructed_price = last_actual_price_for_reconstruction
        for log_ret_pred_step in predicted_log_returns_for_window:
            log_ret_pred_step = np.clip(log_ret_pred_step, -5, 5) 
            current_reconstructed_price = current_reconstructed_price * np.exp(log_ret_pred_step)
            reconstructed_price_forecasts_this_window.append(current_reconstructed_price)
        
        # ä¿®å¤: ç¡®ä¿å®é™…ä»·æ ¼çª—å£ä¸è¶…å‡ºæ•°æ®è¾¹ç•Œ
        end_idx = min(i + pred_len, len(original_prices))
        actual_price_levels_this_window = original_prices[i : end_idx]
        
        # ä¿®å¤: åªæœ‰å½“å®é™…æ•°æ®é•¿åº¦ç­‰äºpred_lenæ—¶æ‰æ·»åŠ åˆ°ç»“æœä¸­
        if len(actual_price_levels_this_window) == pred_len and len(reconstructed_price_forecasts_this_window) == pred_len:
            all_predicted_price_levels_collected.append(reconstructed_price_forecasts_this_window)
            all_true_price_levels_collected.append(actual_price_levels_this_window)
            
            # åªæ”¶é›†æ¯ä¸ªçª—å£çš„ç¬¬ä¸€æ­¥é¢„æµ‹æ®‹å·®
            first_step_residual = reconstructed_price_forecasts_this_window[0] - actual_price_levels_this_window[0]
            all_price_level_residuals.append(first_step_residual)
        else:
            print(f"è­¦å‘Š: è·³è¿‡é•¿åº¦ä¸åŒ¹é…çš„çª—å£ - å®é™…æ•°æ®é•¿åº¦: {len(actual_price_levels_this_window)}, é¢„æµ‹é•¿åº¦: {len(reconstructed_price_forecasts_this_window)}, è¦æ±‚é•¿åº¦: {pred_len}")

    if not all_predicted_price_levels_collected:
        return np.array([]), np.array([]), None, None

    # å¯¹æ±‡ç‡å°ºåº¦çš„æ®‹å·®è¿›è¡Œå®Œæ•´çš„LBæ£€éªŒ
    if len(all_price_level_residuals) > 0:
        residuals_array = np.array(all_price_level_residuals)
        lb_tests = {}
        for lag in [5, 10, 15, 20]:
            lb_test = acorr_ljungbox(residuals_array, lags=[lag], return_df=True)
            lb_tests[f'lag_{lag}'] = {
                'statistic': float(lb_test['lb_stat'].values[0]),
                'p_value': float(lb_test['lb_pvalue'].values[0])
            }
        
        lb_test_result = {
            'ljung_box_tests': lb_tests,
            'residuals_summary': {
                'mean': float(np.mean(residuals_array)),
                'std': float(np.std(residuals_array)),
                'n_samples': len(residuals_array),
                'skewness': float(stats.skew(residuals_array)),
                'kurtosis': float(stats.kurtosis(residuals_array, fisher=True) + 3)
            }
        }
    else:
        lb_test_result = None

    # ä¿®å¤: è¿æ¥æ•°ç»„å‰ç¡®ä¿é•¿åº¦ä¸€è‡´æ€§
    true_values = np.concatenate(all_true_price_levels_collected)
    pred_values = np.concatenate(all_predicted_price_levels_collected)
    
    # ä¿®å¤: æœ€ç»ˆå®‰å…¨æ£€æŸ¥ï¼Œç¡®ä¿é•¿åº¦ä¸€è‡´
    min_length = min(len(true_values), len(pred_values))
    if len(true_values) != len(pred_values):
        print(f"è­¦å‘Š: æœ€ç»ˆæ•°ç»„é•¿åº¦ä¸ä¸€è‡´ï¼Œæˆªå–åˆ°é•¿åº¦: {min_length}")
        true_values = true_values[:min_length]
        pred_values = pred_values[:min_length]

    return true_values, pred_values, first_window_model_info, lb_test_result

def train_and_forecast_pure_arma(scaled_log_return_series_history, seq_len, pred_len, p_ar=1, q_ma=0, save_params=False):
    train_seq = scaled_log_return_series_history[-seq_len:]
    model_params = None
    lb_test_result = None
    
    arima_model = ARIMA(train_seq, order=(p_ar, 0, q_ma))
    try:
        arima_results = arima_model.fit()
        
        if save_params:
            model_params = {
                'mean_equation': {
                    'ar': arima_results.arparams.tolist() if hasattr(arima_results, 'arparams') else [],
                    'ma': arima_results.maparams.tolist() if hasattr(arima_results, 'maparams') else [],
                    'const': float(arima_results.params[0]) if len(arima_results.params) > 0 else 0.0
                }
            }
            # è¿›è¡Œå®Œæ•´çš„LBæ£€éªŒ
            residuals = arima_results.resid
            lb_tests = {}
            for lag in [5, 10, 15, 20]:
                lb_test = acorr_ljungbox(residuals, lags=[lag], return_df=True)
                lb_tests[f'lag_{lag}'] = {
                    'statistic': float(lb_test['lb_stat'].values[0]),
                    'p_value': float(lb_test['lb_pvalue'].values[0])
                }
            
            lb_test_result = {
                'ljung_box_tests': lb_tests,
                'residuals_summary': {
                    'mean': float(np.mean(residuals)),
                    'std': float(np.std(residuals)),
                    'n_samples': len(residuals),
                    'skewness': float(stats.skew(residuals)),
                    'kurtosis': float(stats.kurtosis(residuals, fisher=True) + 3)
                }
            }
        
        forecast_val = arima_results.forecast(steps=pred_len)
        if isinstance(forecast_val, pd.Series): # Handles Series output
            forecast_val = forecast_val.values
        # Ensure it's a 1D array matching pred_len
        return np.array(forecast_val).flatten()[:pred_len], model_params, lb_test_result
    except Exception as e_arima:
        print(f"è­¦å‘Š: ARMA({p_ar},0,{q_ma}) æ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e_arima}ã€‚è¿”å›é›¶é¢„æµ‹ã€‚")
        return np.zeros(pred_len), None, None

def rolling_forecast_pure_arma(data_df, original_price_col_name, log_return_col_name, 
                     seq_len, pred_len, step_size=1, p=1, q=0, test_set_ratio=0.15):
    log_returns = data_df[log_return_col_name].values
    original_prices = data_df[original_price_col_name].values

    log_return_scaler = StandardScaler()
    scaled_log_returns = log_return_scaler.fit_transform(log_returns.reshape(-1, 1)).flatten()
    
    num_total_log_points = len(scaled_log_returns)
    fixed_test_set_size = int(num_total_log_points * test_set_ratio)
    test_start_idx_log = num_total_log_points - fixed_test_set_size
    
    if test_start_idx_log < 0:
        return np.array([]), np.array([]), None, None

    all_true_price_levels_collected = []
    all_predicted_price_levels_collected = []
    first_window_model_info = None
    all_price_level_residuals = []  # æ”¶é›†æ±‡ç‡å°ºåº¦çš„æ®‹å·®ï¼ˆæ¯ä¸ªçª—å£åªå–ç¬¬ä¸€æ­¥é¢„æµ‹çš„æ®‹å·®ï¼‰
    
    extreme_value_threshold = 10.0 # åŒ GARCH rolling forecast ä¸€è‡´çš„é˜ˆå€¼

    for i in tqdm(range(test_start_idx_log, num_total_log_points - pred_len + 1, step_size),
                 desc=f"Pure ARMA({p},{q}) é¢„æµ‹è¿›åº¦ ({pred_len}å¤©)", file=sys.stderr):
        
        current_std_log_ret_history = scaled_log_returns[:i]
        last_actual_price_for_reconstruction = original_prices[i-1]

        # åªåœ¨ç¬¬ä¸€ä¸ªçª—å£ä¿å­˜æ¨¡å‹å‚æ•°
        save_params = (i == test_start_idx_log)
        predicted_std_log_returns, model_params, lb_test_result_unused = train_and_forecast_pure_arma(
            current_std_log_ret_history, seq_len, pred_len, p_ar=p, q_ma=q, save_params=save_params
        )

        if save_params and model_params is not None:
            first_window_model_info = {
                'model_type': f'Pure ARMA({p},{q})',
                'parameters': model_params,
                'lb_test': lb_test_result_unused
            }
        
        # å¯¹çº¯ARMAçš„é¢„æµ‹ä¹Ÿè¿›è¡Œå¥å…¨æ€§æ£€æŸ¥
        if not (isinstance(predicted_std_log_returns, np.ndarray) and \
                predicted_std_log_returns.shape == (pred_len,) and \
                not np.any(np.isnan(predicted_std_log_returns)) and \
                not np.any(np.isinf(predicted_std_log_returns)) and \
                not np.any(np.abs(predicted_std_log_returns) > extreme_value_threshold)):
            print(f"è­¦å‘Š: Pure ARMA({p},{q}) (åœ¨æ»šåŠ¨çª—å£ç´¢å¼• {i}) è¿”å›äº†æ— æ•ˆ/æç«¯é¢„æµ‹: {predicted_std_log_returns}. ä½¿ç”¨é›¶é¢„æµ‹ä»£æ›¿ã€‚")
            predicted_std_log_returns = np.zeros(pred_len)
        
        predicted_log_returns_for_window = log_return_scaler.inverse_transform(
            np.array(predicted_std_log_returns).reshape(-1, 1)
        ).flatten()

        reconstructed_price_forecasts_this_window = []
        current_reconstructed_price = last_actual_price_for_reconstruction
        for log_ret_pred_step in predicted_log_returns_for_window:
            log_ret_pred_step = np.clip(log_ret_pred_step, -5, 5) 
            current_reconstructed_price = current_reconstructed_price * np.exp(log_ret_pred_step)
            reconstructed_price_forecasts_this_window.append(current_reconstructed_price)
        
        # ä¿®å¤: ç¡®ä¿å®é™…ä»·æ ¼çª—å£ä¸è¶…å‡ºæ•°æ®è¾¹ç•Œ
        end_idx = min(i + pred_len, len(original_prices))
        actual_price_levels_this_window = original_prices[i : end_idx]
        
        # ä¿®å¤: åªæœ‰å½“å®é™…æ•°æ®é•¿åº¦ç­‰äºpred_lenæ—¶æ‰æ·»åŠ åˆ°ç»“æœä¸­
        if len(actual_price_levels_this_window) == pred_len and len(reconstructed_price_forecasts_this_window) == pred_len:
            all_predicted_price_levels_collected.append(reconstructed_price_forecasts_this_window)
            all_true_price_levels_collected.append(actual_price_levels_this_window)
            
            # åªæ”¶é›†æ¯ä¸ªçª—å£çš„ç¬¬ä¸€æ­¥é¢„æµ‹æ®‹å·®
            first_step_residual = reconstructed_price_forecasts_this_window[0] - actual_price_levels_this_window[0]
            all_price_level_residuals.append(first_step_residual)
        else:
            print(f"è­¦å‘Š: è·³è¿‡é•¿åº¦ä¸åŒ¹é…çš„çª—å£ - å®é™…æ•°æ®é•¿åº¦: {len(actual_price_levels_this_window)}, é¢„æµ‹é•¿åº¦: {len(reconstructed_price_forecasts_this_window)}, è¦æ±‚é•¿åº¦: {pred_len}")

    if not all_predicted_price_levels_collected:
        return np.array([]), np.array([]), None, None

    # å¯¹æ±‡ç‡å°ºåº¦çš„æ®‹å·®è¿›è¡Œå®Œæ•´çš„LBæ£€éªŒ
    if len(all_price_level_residuals) > 0:
        residuals_array = np.array(all_price_level_residuals)
        lb_tests = {}
        for lag in [5, 10, 15, 20]:
            lb_test = acorr_ljungbox(residuals_array, lags=[lag], return_df=True)
            lb_tests[f'lag_{lag}'] = {
                'statistic': float(lb_test['lb_stat'].values[0]),
                'p_value': float(lb_test['lb_pvalue'].values[0])
            }
        
        lb_test_result = {
            'ljung_box_tests': lb_tests,
            'residuals_summary': {
                'mean': float(np.mean(residuals_array)),
                'std': float(np.std(residuals_array)),
                'n_samples': len(residuals_array),
                'skewness': float(stats.skew(residuals_array)),
                'kurtosis': float(stats.kurtosis(residuals_array, fisher=True) + 3)
            }
        }
    else:
        lb_test_result = None

    # ä¿®å¤: è¿æ¥æ•°ç»„å‰ç¡®ä¿é•¿åº¦ä¸€è‡´æ€§
    true_values = np.concatenate(all_true_price_levels_collected)
    pred_values = np.concatenate(all_predicted_price_levels_collected)
    
    # ä¿®å¤: æœ€ç»ˆå®‰å…¨æ£€æŸ¥ï¼Œç¡®ä¿é•¿åº¦ä¸€è‡´
    min_length = min(len(true_values), len(pred_values))
    if len(true_values) != len(pred_values):
        print(f"è­¦å‘Š: æœ€ç»ˆæ•°ç»„é•¿åº¦ä¸ä¸€è‡´ï¼Œæˆªå–åˆ°é•¿åº¦: {min_length}")
        true_values = true_values[:min_length]
        pred_values = pred_values[:min_length]

    return true_values, pred_values, first_window_model_info, lb_test_result

def main(args):
    log_return_col = 'log_return_gbp_cny' 
    test_set_ratio = 0.15  # ä½¿ç”¨15%ä½œä¸ºæµ‹è¯•é›†

    try:
        data_df = load_and_prepare_data(args.root_path, args.data_path, args.target, log_return_col)
    except ValueError as e:
        print(f"æ•°æ®åŠ è½½æˆ–é¢„å¤„ç†å¤±è´¥: {e}")
        return

    num_total_points = len(data_df)
    fixed_test_set_size = int(num_total_points * test_set_ratio)
    
    if len(data_df) <= fixed_test_set_size:
        print(f"é”™è¯¯: æ•°æ®ç‚¹æ€»æ•° ({len(data_df)}) ä¸è¶³ä»¥åˆ†å‰²å‡º {fixed_test_set_size} ç‚¹çš„æµ‹è¯•é›†ã€‚")
        return
        
    num_train_points_for_first_window = num_total_points - fixed_test_set_size
    
    if num_train_points_for_first_window < args.seq_len:
        print(f"é”™è¯¯: åˆå§‹è®­ç»ƒæ•°æ®ç‚¹ ({num_train_points_for_first_window}) å°‘äºæ¨¡å‹åºåˆ—é•¿åº¦ ({args.seq_len})ã€‚è¯·å‡å°‘ seq_len æˆ–å¢åŠ æ•°æ®ã€‚")
        return

    if not data_df[log_return_col].empty:
        perform_stationarity_test(data_df[log_return_col], series_name=f"{args.target} çš„å¯¹æ•°æ”¶ç›Šç‡ (GBPCNY)")
    else:
        print("å¯¹æ•°æ”¶ç›Šç‡åºåˆ—ä¸ºç©ºã€‚")
        return

    # åŸºç¡€GARCHæ¶æ„å®šä¹‰
    base_garch_architectures = ['ARCH', 'GARCH', 'EGARCH', 'GARCH-M', 'GJR-GARCH', 'TGARCH']
    multistep_unstable_architectures = ['EGARCH', 'TGARCH']  # å¤šæ­¥é¢„æµ‹ä¸­ä¸ç¨³å®šçš„æ¶æ„
    
    # å‡å€¼æ–¹ç¨‹ç±»å‹
    mean_types = ['AR', 'Const']
    
    def generate_model_configs(architectures, mean_types, arma_params):
        """ç”Ÿæˆå®Œæ•´çš„æ¨¡å‹é…ç½®åˆ—è¡¨"""
        configs = []
        
        # Pure ARMAæ¨¡å‹
        for p, q in arma_params:
            configs.append({
                'name': f'Pure ARMA({p},{q})',
                'type': 'Pure ARMA',
                'p': p, 'q': q,
                'architecture': None,
                'mean': None
            })
        
        # GARCHæ—æ¨¡å‹
        for arch in architectures:
            for mean in mean_types:
                for p, q in arma_params:
                    model_name = f'{arch}(1,1)_{mean}_p{p}q{q}' if arch != 'ARCH' else f'{arch}(1)_{mean}_p{p}q{q}'
                    configs.append({
                        'name': model_name,
                        'type': 'GARCH',
                        'p': p, 'q': q,
                        'architecture': arch,
                        'mean': mean
                    })
        
        return configs
    
    # é¢„æµ‹æ­¥é•¿ï¼š1å¤©ã€24å¤©ã€30å¤©ã€60å¤©ã€90å¤©ã€180å¤©
    pred_lens = [1, 24, 30, 60, 90, 180]  # å®Œæ•´å¤šæ­¥é¢„æµ‹ç‰ˆæœ¬
    
    # ARMAå‚æ•°ç»„åˆï¼š(p=ARé˜¶æ•°, q=MAé˜¶æ•°)
    arma_params = [(1, 0), (1, 1), (2, 0)]  # æ‰©å±•ä¸ºå®Œæ•´çš„å‚æ•°ç»„åˆæµ‹è¯•
    # åºåˆ—é•¿åº¦æµ‹è¯•ï¼šç”¨äºæ»šåŠ¨çª—å£çš„å†å²æ•°æ®é•¿åº¦
    seq_lens_to_test = [1000, 500, 250, 125]  # æ‰©å±•çš„è®­ç»ƒçª—å£å¤§å°
    
    best_overall_metrics = {'MSE': float('inf'), 'model': None, 'p': None, 'q': None, 'seq_len': None, 'metrics': None}
    all_configs_comparison = {}
    
    for current_seq_len in seq_lens_to_test:
        args.seq_len = current_seq_len
        print(f"\n{'#'*80}")
        print(f"æµ‹è¯•æ»šåŠ¨çª—å£å¤§å° (seq_len): {current_seq_len}")
        print(f"{'#'*80}")
    
        if num_train_points_for_first_window < current_seq_len:
            print(f"è­¦å‘Š: åˆå§‹è®­ç»ƒæ•°æ®ç‚¹ ({num_train_points_for_first_window}) å°‘äºå½“å‰åºåˆ—é•¿åº¦ ({current_seq_len})ã€‚è·³è¿‡æ­¤åºåˆ—é•¿åº¦ã€‚")
            continue
            
        # ==========================================
        # å¤šæ­¥é¢„æµ‹å¾ªç¯ï¼šä¸ºæ¯ä¸ªé¢„æµ‹æ­¥é•¿åˆ†åˆ«è¿è¡Œæ‰€æœ‰æ¨¡å‹
        # ==========================================
        for pred_len in pred_lens:
            print(f"\n{'*'*80}")
            print(f"å½“å‰é¢„æµ‹æ­¥é•¿: {pred_len}å¤© (seq_len={current_seq_len})")
            print(f"{'*'*80}")

            # ==========================================
            # åŠ¨æ€æ¨¡å‹é€‰æ‹©ï¼šæ ¹æ®é¢„æµ‹æ­¥é•¿é€‰æ‹©ç¨³å®šçš„æ¨¡å‹æ¶æ„
            # ==========================================
            if pred_len == 1:
                # 1æ­¥é¢„æµ‹ï¼šä½¿ç”¨å…¨éƒ¨æ¨¡å‹æ¶æ„
                architectures_to_use = base_garch_architectures
                print(f"ğŸ“Š 1æ­¥é¢„æµ‹ï¼šä½¿ç”¨å…¨éƒ¨{len(base_garch_architectures)}ä¸ªGARCHæ¶æ„")
            else:
                # å¤šæ­¥é¢„æµ‹ï¼šæ’é™¤ä¸ç¨³å®šçš„æ¶æ„
                architectures_to_use = [arch for arch in base_garch_architectures 
                                      if arch not in multistep_unstable_architectures]
                excluded = [arch for arch in base_garch_architectures 
                          if arch in multistep_unstable_architectures]
                print(f"ğŸ“ {pred_len}æ­¥é¢„æµ‹ï¼šä½¿ç”¨{len(architectures_to_use)}ä¸ªç¨³å®šæ¶æ„ï¼Œå·²æ’é™¤{excluded}ä»¥é¿å…æŠ€æœ¯é™åˆ¶")
            
            # ç”Ÿæˆå½“å‰é¢„æµ‹æ­¥é•¿çš„æ¨¡å‹é…ç½®åˆ—è¡¨
            model_configs = generate_model_configs(architectures_to_use, mean_types, arma_params)
            print(f"ğŸ”¢ æ€»è®¡å°†æµ‹è¯•{len(model_configs)}ä¸ªæ¨¡å‹é…ç½®")

            # ==========================================
            # è¿è¡Œæ‰€æœ‰æ¨¡å‹é…ç½®
            # ==========================================
            print(f"\n{'='*80}")
            print(f"è¿è¡Œå®Œæ•´æ¨¡å‹é›† (seq_len={current_seq_len}, pred_len={pred_len})")
            print(f"{'='*80}")
            
            # ä¸ºæ¯ä¸ªé¢„æµ‹æ­¥é•¿åˆ›å»ºå•ç‹¬çš„ç»“æœç›®å½•
            results_dir = f'arima_garch_results_logret_GBPCNY_seq{current_seq_len}_{pred_len}step_last{int(test_set_ratio*100)}pct_test'
            os.makedirs(results_dir, exist_ok=True)
            
            run_results = {}
            metrics_dict = {}
            
            # Naive Baseline æœ´ç´ åŸºçº¿æ¨¡å‹
            print(f"\n{'-'*50}\næ¨¡å‹: Naive Baseline (PrevDay) - {pred_len}æ­¥é¢„æµ‹\n{'-'*50}")
            start_time_naive = time.time()
            naive_actuals, naive_preds = run_naive_baseline_forecast(
                data_df, args.target, pred_len, args.step_size, test_set_ratio
            )
            elapsed_time_naive = time.time() - start_time_naive

            if len(naive_actuals) > 0:
                eval_metrics_naive = evaluate_model(naive_actuals, naive_preds)
                run_results["Naive Baseline (PrevDay)"] = {
                    'metrics': eval_metrics_naive, 'true_values': naive_actuals,
                    'predictions': naive_preds, 'time': elapsed_time_naive,
                    'lb_test': None  # æœ´ç´ åŸºçº¿ä¸åšLBæ£€éªŒ
                }
                metrics_dict["Naive Baseline (PrevDay)"] = eval_metrics_naive
                print(f"æ‰§è¡Œæ—¶é—´: {elapsed_time_naive:.2f}ç§’")
                for name, val_metric in eval_metrics_naive.items():
                    print(f"{name}: {val_metric:.8f}{'%' if name == 'MAPE' else ''}")

            # ==========================================
            # ç»Ÿä¸€çš„æ¨¡å‹é…ç½®å¾ªç¯
            # ==========================================
            for config in model_configs:
                model_name = config['name']
                model_type = config['type']
                p, q = config['p'], config['q']
                architecture = config['architecture']
                mean = config['mean']
                
                print(f"\n{'-'*50}\næ¨¡å‹: {model_name} - {pred_len}æ­¥é¢„æµ‹\n{'-'*50}")
                start_time = time.time()
                
                if model_type == 'Pure ARMA':
                    # Pure ARMAæ¨¡å‹
                    actuals, preds, first_window_model_info, lb_test_result = rolling_forecast_pure_arma(
                        data_df, args.target, log_return_col, current_seq_len, pred_len, 
                        args.step_size, p, q, test_set_ratio=test_set_ratio
                    )
                else:
                    # GARCHæ—æ¨¡å‹ - æ„é€ ä¼ ç»Ÿçš„æ¨¡å‹ç±»å‹å­—ç¬¦ä¸²
                    if architecture == 'ARCH':
                        legacy_model_type = f'{architecture}(1)_{mean}'
                    else:
                        legacy_model_type = f'{architecture}(1,1)_{mean}'
                    
                    actuals, preds, first_window_model_info, lb_test_result = rolling_forecast(
                        data_df, args.target, log_return_col, legacy_model_type,
                        current_seq_len, pred_len, args.step_size, p=p, q=q, test_set_ratio=test_set_ratio
                    )
                
                elapsed_time = time.time() - start_time
                
                if len(actuals) > 0:
                    eval_metrics = evaluate_model(actuals, preds)
                    run_results[model_name] = {
                        'metrics': eval_metrics, 'true_values': actuals,
                        'predictions': preds, 'time': elapsed_time,
                        'lb_test': lb_test_result, 'first_window_info': first_window_model_info
                    }
                    metrics_dict[model_name] = eval_metrics
                    print(f"æ‰§è¡Œæ—¶é—´: {elapsed_time:.2f}ç§’")
                    for name, val_metric in eval_metrics.items():
                        print(f"{name}: {val_metric:.8f}{'%' if name == 'MAPE' else ''}")
                    
                    # è¾“å‡ºLjung-Boxæ£€éªŒç»“æœ
                    if lb_test_result and 'ljung_box_tests' in lb_test_result:
                        print(f"\nLjung-Boxæ£€éªŒç»“æœ:")
                        for lag, test_result in lb_test_result['ljung_box_tests'].items():
                            print(f"  {lag}: ç»Ÿè®¡é‡={test_result['statistic']:.4f}, på€¼={test_result['p_value']:.4f}")
                else:
                    print(f"è­¦å‘Š: {model_name} æ¨¡å‹æ— æœ‰æ•ˆé¢„æµ‹ç»“æœ")

            # ==========================================
            # ä¿å­˜ç»“æœå’Œç”ŸæˆæŠ¥å‘Š
            # ==========================================
            if run_results:
                # ä¿å­˜æ¨¡å‹å‚æ•°åˆ°JSONæ–‡ä»¶
                model_params_file = os.path.join(results_dir, f'model_params_GBPCNY_seq{current_seq_len}_{pred_len}step.json')
                model_params_dict = {}
                for model_name, result_data in run_results.items():
                    if 'first_window_info' in result_data and result_data['first_window_info'] is not None:
                        model_params_dict[model_name] = result_data['first_window_info']
                
                if model_params_dict:
                    with open(model_params_file, 'w', encoding='utf-8') as f:
                        json.dump(model_params_dict, f, indent=4, ensure_ascii=False)
                
                # ä¿å­˜å®Œæ•´ç»“æœæ•°æ®
                with open(os.path.join(results_dir, f'results_GBPCNY_seq{current_seq_len}_pred_len_{pred_len}.pkl'), 'wb') as f:
                    pickle.dump(run_results, f)
                
                # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
                summary_input_for_table = {pred_len: run_results}
                generate_summary_table(summary_input_for_table, results_dir, f"seq{current_seq_len}_{pred_len}step")

            # æ›´æ–°å…¨å±€é…ç½®æ¯”è¾ƒç»“æœ
            config_key = f"seq{current_seq_len}_{pred_len}step"
            if metrics_dict:
                all_configs_comparison[config_key] = {
                    'best_model_name': min(metrics_dict, key=lambda k: metrics_dict[k]['MSE']),
                    'best_mse': min(m['MSE'] for m in metrics_dict.values()),
                    'all_metrics_in_config': metrics_dict,
                    'pred_len': pred_len
                }
                
                # æ›´æ–°å…¨å±€æœ€ä½³æ¨¡å‹
                best_model_in_this_config = min(metrics_dict, key=lambda k: metrics_dict[k]['MSE'])
                best_mse_in_this_config = metrics_dict[best_model_in_this_config]['MSE']
                
                if best_mse_in_this_config < best_overall_metrics['MSE']:
                    # ä»æ¨¡å‹åç§°ä¸­æå–på’Œqå‚æ•°
                    p_extracted = None
                    q_extracted = None
                    if 'p' in model_configs[0]:  # å‡è®¾æ‰€æœ‰é…ç½®éƒ½æœ‰ç›¸åŒçš„p, qè®¾ç½®
                        for config in model_configs:
                            if config['name'] == best_model_in_this_config:
                                p_extracted = config['p']
                                q_extracted = config['q']
                                break
                    
                    best_overall_metrics.update({
                        'MSE': best_mse_in_this_config,
                        'model': f'{best_model_in_this_config} - {pred_len}æ­¥',
                        'p': p_extracted,
                        'q': q_extracted,
                        'seq_len': current_seq_len,
                        'pred_len': pred_len,
                        'metrics': metrics_dict[best_model_in_this_config]
                    })

    # ==========================================
    # 3. å¤šæ­¥é¢„æµ‹ç»“æœæ€»ç»“
    # ==========================================
    print("\n" + "="*100)
    print("å¤šæ­¥é¢„æµ‹å®éªŒå®Œæˆ - æ‰€æœ‰é…ç½®å’Œé¢„æµ‹æ­¥é•¿çš„æ€»ç»“:")
    print("="*100)
    
    # æŒ‰é¢„æµ‹æ­¥é•¿åˆ†ç»„æ˜¾ç¤ºç»“æœ
    results_by_pred_len = {}
    for config_label, summary_res in all_configs_comparison.items():
        pred_len = summary_res.get('pred_len', 1)
        if pred_len not in results_by_pred_len:
            results_by_pred_len[pred_len] = []
        results_by_pred_len[pred_len].append((config_label, summary_res))
    
    for pred_len in sorted(results_by_pred_len.keys()):
        print(f"\n{'-'*80}")
        print(f"é¢„æµ‹æ­¥é•¿: {pred_len}å¤©")
        print(f"{'-'*80}")
        
        configs_for_this_pred_len = results_by_pred_len[pred_len]
        configs_for_this_pred_len.sort(key=lambda x: x[1]['best_mse'])  # æŒ‰MSEæ’åº
        
        for rank, (config_label, summary_res) in enumerate(configs_for_this_pred_len, 1):
            print(f"  {rank}. é…ç½®: {config_label}")
            print(f"     æœ€ä½³æ¨¡å‹: {summary_res['best_model_name']}")
            print(f"     æœ€ä½MSE: {summary_res['best_mse']:.8f}")
            print()
    
    print("\n" + "="*100)
    print("å…¨å±€æœ€ä½³æ¨¡å‹é…ç½® (æ‰€æœ‰é¢„æµ‹æ­¥é•¿ä¸­MSEæœ€ä½çš„æ¨¡å‹):")
    print("="*100)
    if best_overall_metrics['metrics']:
        print(f"æ¨¡å‹ç±»å‹: {best_overall_metrics['model']}")
        if best_overall_metrics['p'] is not None:
            print(f"ARMAå‚æ•°: p={best_overall_metrics['p']}, q={best_overall_metrics['q']}")
        print(f"æ»šåŠ¨çª—å£å¤§å°: {best_overall_metrics['seq_len']}")
        print(f"é¢„æµ‹æ­¥é•¿: {best_overall_metrics.get('pred_len', 1)}å¤©")
        print(f"\nè¯„ä¼°æŒ‡æ ‡:")
        for metric_name, value in best_overall_metrics['metrics'].items():
            print(f"  {metric_name}: {value:.8f}{'%' if metric_name == 'MAPE' else ''}")
    else:
        print("æœªèƒ½æ‰¾åˆ°æœ€ä½³æ¨¡å‹ã€‚")
    
    print("\n" + "="*100)
    print("å®éªŒè¯´æ˜:")
    print("="*100)
    print("1. æ¨¡å‹ç±»å‹:")
    print("   - Naive Baseline: ä½¿ç”¨å‰ä¸€å¤©çš„å€¼ä½œä¸ºæ‰€æœ‰æœªæ¥é¢„æµ‹")
    print("   - Pure ARMA: çº¯æ—¶é—´åºåˆ—è‡ªå›å½’ç§»åŠ¨å¹³å‡æ¨¡å‹")
    print("   - GARCHæ—æ¨¡å‹: åŒ…æ‹¬ARCHã€GARCHã€EGARCHã€GARCH-Mç­‰å¼‚æ–¹å·®æ¨¡å‹")
    print("2. é¢„æµ‹æ­¥é•¿: 1å¤©ã€24å¤©ã€30å¤©ã€60å¤©ã€90å¤©ã€180å¤©çš„å¤šæ­¥é¢„æµ‹")
    print("3. è¯„ä¼°æŒ‡æ ‡: MSEã€RMSEã€MAEã€MAX_AEã€MAPEã€RÂ²")
    print("4. æµ‹è¯•è®¾ç½®: å›ºå®šæœ€å150ä¸ªæ•°æ®ç‚¹ä½œä¸ºæµ‹è¯•é›†")
    print("5. è¯Šæ–­æ£€éªŒ: Ljung-Boxæ£€éªŒç”¨äºæ®‹å·®åºåˆ—ç›¸å…³æ€§æ£€éªŒ")
    print("="*100)



# å›¾è¡¨ç»˜åˆ¶åŠŸèƒ½å·²ç¦ç”¨

def generate_summary_table(all_results_summary, results_dir_path, arima_params_label):
    summary_data = {'Model': []}
    metric_names = ['MSE', 'RMSE', 'MAE', 'MAX_AE', 'MAPE', 'R2', 'Time(s)']  # ç¡®ä¿åŒ…å«MAX_AE
    
    pred_lengths_present = sorted(all_results_summary.keys())
    if not pred_lengths_present: return pd.DataFrame()

    for pred_len_val in pred_lengths_present:
        for metric_n in metric_names:
            summary_data[f'{metric_n}_{pred_len_val}'] = []
    
    model_types_present_set = set()
    for pred_len_val in pred_lengths_present:
        if isinstance(all_results_summary[pred_len_val], dict):
            model_types_present_set.update(all_results_summary[pred_len_val].keys())
    
    ordered_model_types = []
    if "Naive Baseline (PrevDay)" in model_types_present_set: ordered_model_types.append("Naive Baseline (PrevDay)")
    
    pure_arma_keys = sorted([k for k in model_types_present_set if "Pure ARMA" in k])
    ordered_model_types.extend(pure_arma_keys)

    other_garch_models = sorted([k for k in model_types_present_set if k not in ordered_model_types])
    ordered_model_types.extend(other_garch_models)

    if not ordered_model_types: return pd.DataFrame()

    for model_t in ordered_model_types:
        summary_data['Model'].append(model_t)
        for pred_len_val in pred_lengths_present:
            model_result_data = all_results_summary.get(pred_len_val, {}).get(model_t)
            if model_result_data and 'metrics' in model_result_data and 'time' in model_result_data:
                for metric_n in metric_names:
                    val = model_result_data['time'] if metric_n == 'Time(s)' else model_result_data['metrics'].get(metric_n, float('nan'))
                    
                    if metric_n == 'Time(s)': 
                        formatted_val = f"{val:.2f}"  # æ‰§è¡Œæ—¶é—´ä¿æŒ2ä½å°æ•°
                    else:
                        formatted_val = f"{val:.8f}{'%' if metric_n == 'MAPE' else ''}"  # æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ç»Ÿä¸€8ä½å°æ•°
                    summary_data[f'{metric_n}_{pred_len_val}'].append(formatted_val)
            else:
                for metric_n in metric_names:
                    summary_data[f'{metric_n}_{pred_len_val}'].append('N/A')
    
    df_summary = pd.DataFrame(summary_data)
    df_summary.to_csv(os.path.join(results_dir_path, f'summary_table_{arima_params_label}.csv'), index=False)
    print(f"\nç»“æœæ±‡æ€» ({arima_params_label} - å›ºå®š150ç‚¹æµ‹è¯•é›†, é¢„æµ‹é•¿åº¦ {pred_lengths_present[0]}å¤©):")
    pd.set_option('display.float_format', lambda x: '%.8f' if isinstance(x, float) else str(x))  # è®¾ç½®pandasæ˜¾ç¤ºæ ¼å¼ä¸º8ä½å°æ•°
    print(df_summary.to_string())


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ARIMA+GARCHæ±‡ç‡é¢„æµ‹ (GBPCNY, å¯¹æ•°æ”¶ç›Šç‡, å¤šæ­¥é¢„æµ‹, 15%æµ‹è¯•é›†)')
    parser.add_argument('--root_path', type=str, default='./dataset/', help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--data_path', type=str, default='è‹±é•‘å…‘äººæ°‘å¸_short_series.csv', help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--target', type=str, default='rate', help='åŸå§‹æ±‡ç‡ç›®æ ‡å˜é‡åˆ—å')
    parser.add_argument('--seq_len', type=int, default=125, help='ARIMA+GARCHå†å²å¯¹æ•°æ”¶ç›Šç‡é•¿åº¦')
    parser.add_argument('--step_size', type=int, default=1, help='æ»šåŠ¨çª—å£æ­¥é•¿')
    
    args = parser.parse_args()
    main(args) 